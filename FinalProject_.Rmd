---
title: "FinalProject"
author: "Nour El Hamidi, Jonàs Salat Torres"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(forecast)
```

# Exploratory data analysis and transformation into stationarity

```{r}
serie=ts((read.table("eurodol.dat")),start=1995,freq=12)
```

```{r}
plot(serie)
abline(v=1995:2020,col=4,lty=3)
```
There are trends, as the mean is not constant. This is not stationary.

### Is variance constant?

```{r}
boxplot(serie~floor(time(serie)))
```
All boxes should have the same height for variance to be constant, so it's not.

```{r}
m<-apply(matrix(serie,nrow=12),2,mean)
v<-apply(matrix(serie,nrow=12),2,var)
plot(v~m)
abline(lm(v~m),col=2,lty=3)
text(m, v, 1995:2020)
```
There is also an upwards trend for the mean variance plot. The variance is higher for higher values of the mean, so we apply log transformation in order to stabilize the variance.

### Log-transformed series

```{r}
lnserie=log(serie)
plot(lnserie)
```
### Seasonal inspection

```{r}
monthplot(lnserie)
```

The monthplot suggests mild seasonal fluctuations.

```{r}
ts.plot(matrix(lnserie,nrow=12),col=1:8)
```
However, the seasonal matrix plot does not show a stable seasonal pattern across years.
Unlike the example shown in the course, the seasonal matrix plot does not reveal a stable and repeating seasonal pattern across years. The curves are not parallel and no common peak or trough is observed.
Therefore, seasonality appears weak and no seasonal differencing is applied.

### Stationarity of the mean

```{r}
par(mfrow = c(1,2))

acf(lnserie,
    lag.max = 60,
    main = "ACF of log(EUR/USD)")

pacf(lnserie,
     lag.max = 60,
     main = "PACF of log(EUR/USD)")

par(mfrow = c(1,1))

```
The ACF of the log-transformed series declines slowly over many lags, which indicates non-stationarity in the mean. 
Moreover, no significant peaks are observed at seasonal lags(corresponding to 12, 24 or 36 months), suggesting the absence of a seasonal component.
The PACF shows a strong spike at lag 1 followed by mostly insignificant values, which, together with the slowly decaying ACF, is consistent with a non-stationary process in the mean.

Therefore, a regular difference is applied.

It is worth noting that the transformation $(1-B) log(X_t)$ corresponds to a log-return transformation. Indeed, 
$$
log(X_t) - log(X_{t-1}) = log(1 + \frac{X_t - X_{t-1}}{X_{t-1}})
$$ 
and using a first order Taylor expansion, this quantity is approximately equal to 
$$
\frac{X_t - X_{t-1}}{X_{t-1}}
$$
Therefore, working with a differenced log-series is equivalent to modeling log-returns, which is standard for financial time series.

### Regular differencing
```{r}
dlnserie <- diff(lnserie)

plot(dlnserie)
abline(h = 0)
abline(h = mean(dlnserie), col = 2)
```

```{r}
acf(dlnserie,
lag.max = 60,
main = "ACF of differenced log series")
```


```{r}
pacf(dlnserie,
lag.max = 60,
main = "PACF of differenced log series")
```

After one regular difference, the series fluctuates around a constant mean.
The ACF cuts off rapidly, indicating that stationarity in the mean has been achieved.
The PACF displays only a small number of significant initial lags followed by values close to zero, which is consistent with a stationary process.


### Variance checking 
```{r}
og_var <- var(serie)
log_var <- var(lnserie)
dln_var <- var(dlnserie)

og_var 
log_var
dln_var
```
The variance of the original series is higher than the variance of the log-transformed series, indicating that the logarithmic transformation stabilizes the variance. After regular differencing, the variance remains stable, confirming that no further variance-stabilizing transformation is required.

# Model identification

The identification of the ARIMA model is based on the ACF and PACF of the stationary series $dln(Xt)$. So $$ X_t \sim \text{ARIMA}(p, 1, q) $$

The ACF shows a significant spike at lag 1 followed by values close to zero, while the PACF also presents a significant lag at 1.This suggests that low-order AR and MA components may be present. Consequently, ARIMA(1,1,0) and ARIMA(0,1,1) are considered as plausible candidate models.

### Model estimation
```{r}
m1 <- arima(lnserie, order = c(1,1,0))
m2 <- arima(lnserie, order = c(0,1,1))

m1
m2

```
```{r}
coeftest <- function(model){
  tstat <- model$coef / sqrt(diag(model$var.coef))
  cbind(Estimate = model$coef,
        "t value" = tstat,
        Significant = abs(tstat) > 2)
}

coeftest(m1)
coeftest(m2)
```

Two candidate models were estimated: 
  - ARIMA(1,1,0), 
  - ARIMA(0,1,1).

In the ARIMA(1,1,0) model, the autoregressive parameter is significant, but the model exhibits a higher AIC than the alternative.

The ARIMA(0,1,1) model presents a highly significant moving-average coefficient and achieves the lowest AIC among the considered models. 

Thus, ARIMA(0,1,1) is selected as the best model.

# Validation 
According to the Box–Jenkins methodology, a model is considered appropriate if its residuals behave like white noise. In particular, the residuals should have zero mean, constant variance, follow an approximately normal distribution, and exhibit no autocorrelation. These assumptions are examined using both graphical tools and formal statistical tests.

##Residual Analysis

### 0-mean ?
```{r}
resid2 <- residuals(m2)
```

```{r}
par(mfrow = c(1,2))
plot(resid2, main = "Residuals ARIMA(0, 1, 1)")
abline(h = 0, col = "red")
plot(sqrt(abs(resid2)), main = "Sqrt(|Residuals|) ARIMA(0, 1, 1)")
```
The residuals fluctuate randomly around zero with no visible change in dispersion over time. This suggests that the variance of the residuals is approximately constant.

### Normality of the Residuals
```{r}
par(mfrow = c(1,2))

qqnorm(resid2)
qqline(resid2, col = 2)

hist(resid2, breaks = 15, freq = FALSE)
curve(dnorm(x, mean(resid2), sd(resid2)), col = 2, add = TRUE)
```
The Q–Q plot and the histogram indicate that the residuals are reasonably close to a normal distribution.

### Independance of the Residuals
```{r}
par(mfrow = c(1,2))
acf(resid2, lag.max = 60)
pacf(resid2, lag.max = 60)
```
Those results were similar for the ARIMA(1, 1, 0) model.

### Ljung–Box Test

```{r}
Box.test(resid2, lag = 24, type = "Ljung-Box")

resid1 <- residuals(m1)
Box.test(resid1, lag = 24, type = "Ljung-Box")
```
The ACF and PACF of the residuals do not exhibit significant autocorrelations.
Furthermore, the Ljung–Box test fails to reject the null hypothesis of no autocorrelation, confirming that the residuals behave as white noise.


## AR and MA Infinite Representations

### Infinite MA Representation (Casuality)
For ARIMA(0, 1, 1) :
$$
(1-B)X_t = (1+ \theta B) Z_t
$$
with $\hat{\theta} = 0.327$.
Since the estimated moving-average parameter satisfies $|\hat{\theta}| < 1$, the model admits an infinite MA representation. This means that the influence of past innovations decreases over time, ensuring a stable dynamic behavior of the model.


### Infinite AR Representation (Invertibility)
The invertibility condition is satisfied because the estimated MA parameter has modulus smaller than one. Therefore, the model admits an infinite AR representation, allowing the recovery of the innovations from past observations.


### Model Adequacy Measures
```{r}
AIC(m1)
BIC(m1)

AIC(m2)
BIC(m2)
```
Model adequacy is assessed using Akaike and Bayesian information criteria. The selected model achieves the lowest AIC among the candidate specifications, indicating a good balance between fit and parsimony.

## Stability and Predictive Capability

The estimated parameters of the ARIMA(0,1,1) model satisfy the causality and invertibility conditions. As a result, the model is stable and suitable for forecasting.

```{r}
end_train <- time(lnserie)[length(lnserie) - 12]
start_test <- time(lnserie)[length(lnserie) - 11]

train <- window(lnserie, end = end_train)
test  <- window(lnserie, start = start_test)


model_cv1 <- arima(train, order = c(0,1,1))
fc_cv1 <- forecast(model_cv1, h = 12)

accuracy(fc_cv1$mean, test)
```
```{r}
model_cv2 <- arima(train, order = c(1,1,0))
fc_cv2 <- forecast(model_cv2, h = 12)

accuracy(fc_cv2$mean, test)
```

To assess predictive capability, the last 12 observations were reserved as a validation sample and compared with 12-step-ahead forecasts.
The ARIMA(0,1,1) and ARIMA(1,1,0) models exhibit very similar forecast accuracy measures in terms of RMSE, MAE and MAPE, indicating comparable short-term predictive performance.

For both models, the mean error (ME) is close to zero, indicating the absence of a strong systematic bias in the forecasts. The root mean squared error (RMSE) and mean absolute error (MAE) are of similar magnitude, suggesting that prediction errors are generally moderate and not driven by a small number of large deviations.

The mean absolute percentage error (MAPE) is relatively high, which is common for financial time series and can be partly explained by the logarithmic differencing, where small denominators may inflate relative error measures. The first-order autocorrelation of the forecast errors (ACF1) is positive, reflecting some short-term dependence in the out-of-sample errors, although this result should be interpreted with caution given the small size of the validation sample.

Finally, Theil’s U statistic exceeds one, indicating that the model does not systematically outperform a naive benchmark over the validation period. This result highlights the inherent difficulty of forecasting exchange rates rather than a deficiency of the model. Overall, the ARIMA(0,1,1) model provides reasonable short-term forecasts, while clearly illustrating the limits of predictability in financial time series.

### Model Selection

Given the small size of the validation sample, these differences should not be over-interpreted. Since ARIMA(0,1,1) provided a better in-sample fit, with a lower AIC and only significant parameters, it is retained as the preferred model according to the principle of parsimony.


## Prediction

```{r}
ultim=c(2018,12)
pdq=c(0,1,1)

```


```{r}
pred=predict(m2,n.ahead=12)
pr<-ts(c(tail(lnserie,1),pred$pred),start=ultim+c(1,0),freq=12)
se<-ts(c(0,pred$se),start=ultim+c(1,0),freq=12)

#Intervals
tl1<-ts(exp(pr-1.96*se),start=ultim+c(1,0),freq=12)
tu1<-ts(exp(pr+1.96*se),start=ultim+c(1,0),freq=12)
pr1<-ts(exp(pr),start=ultim+c(1,0),freq=12)

ts.plot(serie,tl1,tu1,pr1,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=c(ultim[1]-2,ultim[1]+3),type="o",main=paste("Model ARIMA(",paste(pdq,collapse=","),")",sep=""))
abline(v=(ultim[1]-2):(ultim[1]+3),lty=3,col=4)
```

```{r}
(previs1=window(cbind(tl1,pr1,tu1),start=ultim+c(1,0)))
```
After the last observation, the ARIMA(0,1,1) forecast is essentially flat, staying close to the most recent level (≈ 1.10–1.12).
The bounds fan out as you go further into the future. This means forecast uncertainty increases with horizon. By the end of the 12-month horizon, the interval is quite wide (roughly ~0.9 to ~1.35 on the plot), so while the central prediction is stable, the model allows for meaningful upside or downside movement.
Since we don't have seasonality, short-term forecasts (next 1–3 months) are relatively tight and more informative; the further out you go, the interval becomes wide enough that the forecast is more “range guidance” than a precise point estimate.